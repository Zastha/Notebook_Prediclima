{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8598dcf3",
   "metadata": {},
   "source": [
    "# Entrenamiento XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054e95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.15.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n",
    "! pip install scikit-learn\n",
    "! pip install pandas\n",
    "! pip install numpy \n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b73465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(\"XGBoost version:\", xgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366754c",
   "metadata": {},
   "source": [
    "## Division de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c182383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando train data en ./XGBOOST_dataset_splits/Seco/NR_25037/train\n",
      "Guardando test data en ./XGBOOST_dataset_splits/Seco/NR_25037/test\n",
      "Guardando val data en ./XGBOOST_dataset_splits/Seco/NR_25037/val\n",
      "Guardando train data en ./XGBOOST_dataset_splits/Templado/NR_25033/train\n",
      "Guardando test data en ./XGBOOST_dataset_splits/Templado/NR_25033/test\n",
      "Guardando val data en ./XGBOOST_dataset_splits/Templado/NR_25033/val\n",
      "Guardando train data en ./XGBOOST_dataset_splits/Tropical/NR_25046/train\n",
      "Guardando test data en ./XGBOOST_dataset_splits/Tropical/NR_25046/test\n",
      "Guardando val data en ./XGBOOST_dataset_splits/Tropical/NR_25046/val\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_dict = {\n",
    "    25037: {'Clasificacion': 'Seco', 'ruta_origen': '../data/data_conagua_clasificada/Seco/NR_25037.csv', 'ruta_salida': './XGBOOST_dataset_splits/Seco/NR_25037/'},\n",
    "    25033: {'Clasificacion': 'Templado', 'ruta_origen': '../data/data_conagua_clasificada/Templado/NR_25033.csv', 'ruta_salida': './XGBOOST_dataset_splits/Templado/NR_25033/'},\n",
    "    25046: {'Clasificacion': 'Humedo', 'ruta_origen': '../data/data_conagua_clasificada/Tropical/NR_25046.csv', 'ruta_salida': './XGBOOST_dataset_splits/Tropical/NR_25046/'}\n",
    "}\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "TEST_RATIO = 0.20\n",
    "\n",
    "for file_id, file_info in file_dict.items():\n",
    "    ruta_origen = file_info['ruta_origen']\n",
    "    ruta_salida_base = file_info['ruta_salida']\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(ruta_origen, index_col=0, parse_dates=True).asfreq('D')\n",
    "    df = df.sort_index()\n",
    "    df = df.asfreq('D')\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    train_end = int(total_rows * TRAIN_RATIO)\n",
    "    test_end = int(total_rows * (TRAIN_RATIO + TEST_RATIO))\n",
    "    \n",
    "    df_train = df.iloc[:train_end]\n",
    "    df_test = df.iloc[train_end:test_end]\n",
    "    df_val = df.iloc[test_end:]\n",
    "    \n",
    "    splits = {'train':{'df': df_train, 'ruta_salida': os.path.join(ruta_salida_base, 'train')},\n",
    "              'test': {'df': df_test, 'ruta_salida': os.path.join(ruta_salida_base, 'test')},\n",
    "              'val':  {'df': df_val, 'ruta_salida': os.path.join    (ruta_salida_base, 'val')}}\n",
    "    \n",
    "    for split_name, split_info in splits.items():\n",
    "        save_path = split_info['ruta_salida']\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"Guardando {split_name} data en {save_path}\")\n",
    "        split_info['df'].to_csv(os.path.join(save_path, f\"{split_name}_data.csv\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5acde5",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecb0e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos de entrenamiento desde ./XGBOOST_dataset_splits/Seco\\NR_25037\\train\\train_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de imputacipn guardado en ./XGBOOST_trained_models/Seco\\NR_25037\n",
      "Cargando datos de entrenamiento desde ./XGBOOST_dataset_splits/Templado\\NR_25033\\train\\train_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de imputacipn guardado en ./XGBOOST_trained_models/Templado\\NR_25033\n",
      "Cargando datos de entrenamiento desde ./XGBOOST_dataset_splits/Tropical\\NR_25046\\train\\train_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sarah\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de imputacipn guardado en ./XGBOOST_trained_models/Tropical\\NR_25046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import joblib\n",
    "\n",
    "directory = './XGBOOST_dataset_splits/'\n",
    "model_output_dir = './XGBOOST_trained_models/'\n",
    "\n",
    "\n",
    "#Hiperparametros\n",
    "params = {\n",
    "    'estimator': XGBRegressor(\n",
    "    n_estimators= 100, #Numero de arboles\n",
    "    learning_rate= 0.05, #Tasa de aprendizaje\n",
    "    max_depth= 5, #Profundidad maxima de los arboles, baja para evitar sobreajuste\n",
    "    n_jobs= -1, #Usar todos los nucleos disponibles\n",
    "    random_state= 42, #Para reproducibilidad\n",
    "    tree_method= 'hist' #Metodo de construccion de arboles eficiente\n",
    "    ),\n",
    "    'max_iter': 10, #Numero maximo de iteraciones para el imputador\n",
    "    'random_state': 42 #Para reproducibilidad\n",
    "}\n",
    "\n",
    "columnas_objetivo = ['PRECIP', 'EVAP', 'TMAX', 'TMIN']\n",
    "\n",
    "estaciones = {\n",
    "    'NR_25037': 'Seco',\n",
    "    'NR_25033': 'Templado',\n",
    "    'NR_25046': 'Tropical'\n",
    "}\n",
    "\n",
    "for region, estacion in estaciones.items():\n",
    "    archivo_train = os.path.join(directory, estacion, region, 'train', 'train_data.csv')\n",
    "    \n",
    "    print(f\"Cargando datos de entrenamiento desde {archivo_train}\")\n",
    "    \n",
    "    df_train = pd.read_csv(archivo_train, index_col=0, parse_dates=True).asfreq('D')\n",
    "    \n",
    "    df = df_train.copy()\n",
    "    df['dayofyear'] = df.index.dayofyear #Feature ciclico para el día del año\n",
    "    df['month'] = df.index.month  #Feature ciclico para el mes\n",
    "    df['sin_dayofyear'] = np.sin(2 * np.pi * df['dayofyear'] / 365.25) #Feature ciclico seno (Necesario para capturar patrones estacionales por distancia entre fecha)\n",
    "    df['cos_dayofyear'] = np.cos(2 * np.pi * df['dayofyear'] / 365.25) #Feature ciclico coseno\n",
    "    \n",
    "    columnas_imputar = columnas_objetivo + ['dayofyear', 'month', 'sin_dayofyear', 'cos_dayofyear']\n",
    "\n",
    "    imputer = IterativeImputer(**params)\n",
    "    imputer.fit(df[columnas_imputar])\n",
    "    \n",
    "    modelo_imputacion_path = os.path.join(model_output_dir, estacion, region)\n",
    "    os.makedirs(modelo_imputacion_path, exist_ok=True)\n",
    "    joblib.dump(imputer, os.path.join(modelo_imputacion_path, 'imputer_model.joblib'))\n",
    "    print(f\"Modelo de imputacipn guardado en {modelo_imputacion_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddd75c",
   "metadata": {},
   "source": [
    "## Testing de imputador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1888d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO VALIDACIÓN DE IMPUTACIÓN ---\n",
      "\n",
      "Procesando Estación: 25037...\n",
      "   Evaluando variable: PRECIP\n",
      "      [OK] Guardado: /PRECIP/metrics_test_PRECIP.csv\n",
      "   Evaluando variable: EVAP\n",
      "      [OK] Guardado: /EVAP/metrics_test_EVAP.csv\n",
      "   Evaluando variable: TMAX\n",
      "      [OK] Guardado: /TMAX/metrics_test_TMAX.csv\n",
      "   Evaluando variable: TMIN\n",
      "      [OK] Guardado: /TMIN/metrics_test_TMIN.csv\n",
      "\n",
      "Procesando Estación: 25033...\n",
      "   Evaluando variable: PRECIP\n",
      "      [OK] Guardado: /PRECIP/metrics_test_PRECIP.csv\n",
      "   Evaluando variable: EVAP\n",
      "      [OK] Guardado: /EVAP/metrics_test_EVAP.csv\n",
      "   Evaluando variable: TMAX\n",
      "      [OK] Guardado: /TMAX/metrics_test_TMAX.csv\n",
      "   Evaluando variable: TMIN\n",
      "      [OK] Guardado: /TMIN/metrics_test_TMIN.csv\n",
      "\n",
      "Procesando Estación: 25046...\n",
      "   Evaluando variable: PRECIP\n",
      "      [OK] Guardado: /PRECIP/metrics_test_PRECIP.csv\n",
      "   Evaluando variable: EVAP\n",
      "      [OK] Guardado: /EVAP/metrics_test_EVAP.csv\n",
      "   Evaluando variable: TMAX\n",
      "      [OK] Guardado: /TMAX/metrics_test_TMAX.csv\n",
      "   Evaluando variable: TMIN\n",
      "      [OK] Guardado: /TMIN/metrics_test_TMIN.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "directory = {\n",
    "    25037:{\n",
    "        'Clasificacion': 'Seco',\n",
    "        'ruta_input_test': './XGBOOST_dataset_splits/Seco/NR_25037/test/test_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Seco/NR_25037/test/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Seco/NR_25037/imputer_model.joblib'\n",
    "    },\n",
    "    25033:{\n",
    "        'Clasificacion': 'Templado',\n",
    "        'ruta_input_test': './XGBOOST_dataset_splits/Templado/NR_25033/test/test_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Templado/NR_25033/test/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Templado/NR_25033/imputer_model.joblib'\n",
    "    },\n",
    "    25046:{\n",
    "        'Clasificacion': 'Tropical',\n",
    "        'ruta_input_test': './XGBOOST_dataset_splits/Tropical/NR_25046/test/test_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Tropical/NR_25046/test/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Tropical/NR_25046/imputer_model.joblib'\n",
    "    },\n",
    "}\n",
    "\n",
    "columnas_objetivo = ['PRECIP', 'EVAP', 'TMAX', 'TMIN']\n",
    "\n",
    "# Horizontes\n",
    "horizontes = [0, 3, 7, 31, 180, 365, 730, float('inf')]\n",
    "etiquetas_horizontes = ['1d', '3d', '7d', '31d', '180d', '365d', '730d', 'full']\n",
    "\n",
    "print(\"--- INICIANDO VALIDACIÓN DE IMPUTACIÓN ---\")\n",
    "\n",
    "for estacion, info in directory.items():\n",
    "    test_path = info['ruta_input_test']\n",
    "    modelo_imputacion_path = info['ruta_modelo_imputacion']\n",
    "    ruta_salida = info['ruta_salida']\n",
    "    os.makedirs(ruta_salida, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nProcesando Estación: {estacion}...\")\n",
    "\n",
    "    if not os.path.exists(test_path):\n",
    "        print(f\"   [SKIP] No existe archivo test: {test_path}\")\n",
    "        continue\n",
    "        \n",
    "    df_test_raw = pd.read_csv(test_path, index_col=0, parse_dates=True).asfreq('D')\n",
    "    \n",
    "    try:\n",
    "        imputer = joblib.load(modelo_imputacion_path)\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] No se pudo cargar el imputer: {e}\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    df_para_imputar = df_test_raw.copy()\n",
    "    df_para_imputar['dayofyear'] = df_para_imputar.index.dayofyear\n",
    "    df_para_imputar['month'] = df_para_imputar.index.month\n",
    "    df_para_imputar['sin_dayofyear'] = np.sin(2 * np.pi * df_para_imputar.index.dayofyear / 365.25)\n",
    "    df_para_imputar['cos_dayofyear'] = np.cos(2 * np.pi * df_para_imputar.index.dayofyear / 365.25)\n",
    "    \n",
    "    columnas_mice = columnas_objetivo + ['dayofyear', 'month', 'sin_dayofyear', 'cos_dayofyear']\n",
    "    \n",
    "    for objetivo in columnas_objetivo:\n",
    "        if objetivo not in df_test_raw.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"   Evaluando variable: {objetivo}\")\n",
    "            \n",
    "        df_mascara = df_para_imputar.copy()\n",
    "        indices_validos = df_mascara[df_mascara[objetivo].notna()].index\n",
    "        \n",
    "        if len(indices_validos) == 0:\n",
    "            print(f\"      No hay datos válidos para evaluar {objetivo}. Saltando...\")\n",
    "            continue\n",
    "        \n",
    "        #Guardar Realidad y Borrar Datos\n",
    "        datos_reales = df_mascara.loc[indices_validos, objetivo].copy()\n",
    "        df_mascara.loc[indices_validos, objetivo] = np.nan # Borramos para obligar a imputar\n",
    "        \n",
    "        try:\n",
    "            matriz_imputada = imputer.transform(df_mascara[columnas_mice])\n",
    "            df_imputada = pd.DataFrame(matriz_imputada, columns=columnas_mice, index=df_mascara.index)\n",
    "\n",
    "            y_pred_imputado = df_imputada.loc[indices_validos, objetivo]\n",
    "            \n",
    "            df_resultado = pd.DataFrame({\n",
    "                'Real': datos_reales,\n",
    "                'Imputado': y_pred_imputado\n",
    "            })\n",
    "            \n",
    "            # Calcular Horizontes\n",
    "            fecha_inicio = df_resultado.index.min()\n",
    "            df_resultado['Día'] = (df_resultado.index - fecha_inicio).days + 1\n",
    "            df_resultado['rango'] = pd.cut(df_resultado['Día'], bins=[-1] + horizontes, labels=etiquetas_horizontes)\n",
    "            \n",
    "            lista_metricas = []\n",
    "            \n",
    "            for etiqueta in etiquetas_horizontes:\n",
    "                subset = df_resultado[df_resultado['rango'] == etiqueta]\n",
    "                count = len(subset)\n",
    "                \n",
    "                if count > 0:\n",
    "                    y_true = subset['Real']\n",
    "                    y_pred = subset['Imputado']\n",
    "                    \n",
    "                    mse = np.mean((y_true - y_pred)**2)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = np.mean(np.abs(y_true - y_pred))\n",
    "                    \n",
    "                    # MAPE seguro\n",
    "                    mascara_no_cero = y_true != 0\n",
    "                    if np.sum(mascara_no_cero) > 0:\n",
    "                        mape = np.mean(np.abs((y_true[mascara_no_cero] - y_pred[mascara_no_cero]) / y_true[mascara_no_cero]))\n",
    "                    else:\n",
    "                        mape = 0.0\n",
    "                else:\n",
    "                    mse = 0.0\n",
    "                    rmse = 0.0\n",
    "                    mae = 0.0\n",
    "                    mape = 0.0\n",
    "                \n",
    "                lista_metricas.append({\n",
    "                    'Horizonte': etiqueta,\n",
    "                    'MSE': round(mse, 4),\n",
    "                    'RMSE': round(rmse, 4),\n",
    "                    'MAE': round(mae, 4),\n",
    "                    'MAPE': round(mape, 4),\n",
    "                    'Count': count\n",
    "                })\n",
    "                \n",
    "            \n",
    "            df_metricas = pd.DataFrame(lista_metricas)\n",
    "            archivo_csv_metrics = os.path.join(ruta_salida, f'/{objetivo}/metrics_test_{objetivo}.csv')\n",
    "            os.makedirs(os.path.dirname(archivo_csv_metrics), exist_ok=True)\n",
    "            df_metricas.to_csv(archivo_csv_metrics, index=False)\n",
    "            \n",
    "            # Guardar Imagen Tabla\n",
    "            fig, ax = plt.subplots(figsize=(10, 4))\n",
    "            ax.axis('off')\n",
    "            tabla = ax.table(cellText=df_metricas.values,\n",
    "                             colLabels=df_metricas.columns,\n",
    "                             cellLoc='center',\n",
    "                             loc='center')\n",
    "            tabla.scale(1, 1.5)\n",
    "            tabla.auto_set_font_size(False)\n",
    "            tabla.set_fontsize(9)\n",
    "            \n",
    "            plt.title(f'Validación Imputación: {objetivo} - {estacion}')\n",
    "            plt.savefig(os.path.join(ruta_salida, f'metrics_table_{objetivo}.png'), bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"      [OK] Guardado: {archivo_csv_metrics}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      [ERROR] Falló imputación de {objetivo}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ece8e",
   "metadata": {},
   "source": [
    "## Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94cd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando Estación: 25037\n",
      "   -> Generando reportes para: PRECIP\n",
      "   -> Generando reportes para: EVAP\n",
      "   -> Generando reportes para: TMAX\n",
      "   -> Generando reportes para: TMIN\n",
      "\n",
      "Procesando Estación: 25033\n",
      "   -> Generando reportes para: PRECIP\n",
      "   -> Generando reportes para: EVAP\n",
      "   -> Generando reportes para: TMAX\n",
      "   -> Generando reportes para: TMIN\n",
      "\n",
      "Procesando Estación: 25046\n",
      "   -> Generando reportes para: PRECIP\n",
      "   -> Generando reportes para: EVAP\n",
      "   -> Generando reportes para: TMAX\n",
      "   -> Generando reportes para: TMIN\n",
      "\n",
      "--- PROCESO TERMINADO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "directory = {\n",
    "    25037: {\n",
    "        'Clasificacion': 'Seco',\n",
    "        'ruta_input_val': './XGBOOST_dataset_splits/Seco/NR_25037/val/val_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Seco/NR_25037/Validation_Imputation/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Seco/NR_25037/imputer_model.joblib'\n",
    "    },\n",
    "    25033: {\n",
    "        'Clasificacion': 'Templado',\n",
    "        'ruta_input_val': './XGBOOST_dataset_splits/Templado/NR_25033/val/val_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Templado/NR_25033/Validation_Imputation/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Templado/NR_25033/imputer_model.joblib'\n",
    "    },\n",
    "    25046: {\n",
    "        'Clasificacion': 'Tropical',\n",
    "        'ruta_input_val': './XGBOOST_dataset_splits/Tropical/NR_25046/val/val_data.csv',\n",
    "        'ruta_salida': './data_analysis/Performance_metrics/Tropical/NR_25046/Validation_Imputation/',\n",
    "        'ruta_modelo_imputacion': './XGBOOST_trained_models/Tropical/NR_25046/imputer_model.joblib'\n",
    "    },\n",
    "}\n",
    "\n",
    "columnas_objetivo = ['PRECIP', 'EVAP', 'TMAX', 'TMIN']\n",
    "horizontes = [0, 3, 7, 31, 180, 365, 730, float('inf')]\n",
    "etiquetas_horizontes = ['1d', '3d', '7d', '31d', '180d', '365d', '730d', 'full']\n",
    "\n",
    "\n",
    "for estacion, info in directory.items():\n",
    "    val_path = info['ruta_input_val']\n",
    "    modelo_imputacion_path = info['ruta_modelo_imputacion']\n",
    "    ruta_base_salida = info['ruta_salida']\n",
    "    \n",
    "    print(f\"\\nProcesando Estación: {estacion}\")\n",
    "\n",
    "    if not os.path.exists(val_path):\n",
    "        print(f\"   [SKIP] No existe archivo: {val_path}\")\n",
    "        continue\n",
    "\n",
    "    df_old = pd.read_csv(val_path, index_col=0, parse_dates=True).asfreq('D')\n",
    "    \n",
    "    try:\n",
    "        imputer = joblib.load(modelo_imputacion_path)\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Imputer no cargado: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #Contexto Temporal\n",
    "    df_ctx = df_old.copy()\n",
    "    df_ctx['dayofyear'] = df_ctx.index.dayofyear\n",
    "    df_ctx['month'] = df_ctx.index.month\n",
    "    df_ctx['sin_dayofyear'] = np.sin(2 * np.pi * df_ctx.index.dayofyear / 365.25)\n",
    "    df_ctx['cos_dayofyear'] = np.cos(2 * np.pi * df_ctx.index.dayofyear / 365.25)\n",
    "    \n",
    "    cols_mice = columnas_objetivo + ['dayofyear', 'month', 'sin_dayofyear', 'cos_dayofyear']\n",
    "\n",
    "    try:\n",
    "        matriz_nueva = imputer.transform(df_ctx[cols_mice])\n",
    "        df_new = pd.DataFrame(matriz_nueva, columns=cols_mice, index=df_old.index)\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Falló imputación global: {e}\")\n",
    "        continue\n",
    "\n",
    "    for objetivo in columnas_objetivo:\n",
    "        if objetivo not in df_old.columns: continue\n",
    "        \n",
    "        print(f\"   -> Generando reportes para: {objetivo}\")\n",
    "        \n",
    "\n",
    "        dir_variable = os.path.join(ruta_base_salida, objetivo)\n",
    "        os.makedirs(dir_variable, exist_ok=True)\n",
    "\n",
    "        #Real vs Imputado\n",
    "        df_comp = pd.DataFrame(index=df_old.index)\n",
    "        df_comp['Original'] = df_old[objetivo]\n",
    "        df_comp['Imputado'] = df_new[objetivo]\n",
    "        df_comp['Fue_Rellenado'] = df_old[objetivo].isna()\n",
    "        \n",
    "        # Guardar en la carpeta de la variable\n",
    "        df_comp.to_csv(os.path.join(dir_variable, 'comparativo_old_vs_new.csv'))\n",
    "        \n",
    "       # Log de Cambios\n",
    "        huecos = df_comp[df_comp['Fue_Rellenado'] == True].copy()\n",
    "        if not huecos.empty:\n",
    "            huecos[['Original', 'Imputado']].to_csv(os.path.join(dir_variable, 'log_imputaciones_realizadas.csv'))\n",
    "        \n",
    "       # Validación Métricas\n",
    "        indices_validos = df_ctx[df_ctx[objetivo].notna()].index\n",
    "        \n",
    "        if len(indices_validos) > 0:\n",
    "            df_mask = df_ctx.copy()\n",
    "            ground_truth = df_mask.loc[indices_validos, objetivo].copy()\n",
    "            df_mask.loc[indices_validos, objetivo] = np.nan \n",
    "            \n",
    "            try:\n",
    "                matriz_val = imputer.transform(df_mask[cols_mice])\n",
    "                df_val_res = pd.DataFrame(matriz_val, columns=cols_mice, index=df_mask.index)\n",
    "                y_pred = df_val_res.loc[indices_validos, objetivo]\n",
    "                \n",
    "                df_res = pd.DataFrame({'Real': ground_truth, 'Pred': y_pred})\n",
    "                start = df_res.index.min()\n",
    "                df_res['Dias'] = (df_res.index - start).days + 1\n",
    "                df_res['Rango'] = pd.cut(df_res['Dias'], bins=[-1] + horizontes, labels=etiquetas_horizontes)\n",
    "                \n",
    "                metrics_list = []\n",
    "                for label in etiquetas_horizontes:\n",
    "                    sub = df_res[df_res['Rango'] == label]\n",
    "                    count = len(sub)\n",
    "                    if count > 0:\n",
    "                        mse = np.mean((sub['Real'] - sub['Pred'])**2)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        mae = np.mean(np.abs(sub['Real'] - sub['Pred']))\n",
    "                        \n",
    "                        mask_nz = sub['Real'] != 0\n",
    "                        if mask_nz.sum() > 0:\n",
    "                            mape = np.mean(np.abs((sub.loc[mask_nz, 'Real'] - sub.loc[mask_nz, 'Pred']) / sub.loc[mask_nz, 'Real']))\n",
    "                        else:\n",
    "                            mape = 0.0\n",
    "                    else:\n",
    "                        mse, rmse, mae, mape = 0,0,0,0\n",
    "                    \n",
    "                    metrics_list.append({\n",
    "                        'Horizonte': label, 'MSE': round(mse,4), 'RMSE': round(rmse,4), \n",
    "                        'MAE': round(mae,4), 'MAPE': round(mape,4), 'Count': count\n",
    "                    })\n",
    "                \n",
    "                # Guardar Metricas en la carpeta de la variable\n",
    "                df_m = pd.DataFrame(metrics_list)\n",
    "                df_m.to_csv(os.path.join(dir_variable, 'metricas_validacion.csv'), index=False)\n",
    "                \n",
    "                # Imagen de Tabla\n",
    "                fig, ax = plt.subplots(figsize=(10, 3))\n",
    "                ax.axis('off')\n",
    "                tbl = ax.table(cellText=df_m.values, colLabels=df_m.columns, loc='center', cellLoc='center')\n",
    "                tbl.scale(1, 1.4)\n",
    "                plt.title(f'Validación Tabla: {objetivo}')\n",
    "                plt.savefig(os.path.join(dir_variable, 'tabla_metricas.png'), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                limit_plot = 150\n",
    "                \n",
    "                plt.figure(figsize=(12, 5))\n",
    "                # Graficamos datos reales\n",
    "                plt.plot(df_res['Real'].iloc[:limit_plot].values, label='Real (Validación)', color='blue', alpha=0.4)\n",
    "                # Graficamos datos imputados\n",
    "                plt.plot(df_res['Pred'].iloc[:limit_plot].values, label='Reconstrucción (XGBoost)', color='orange', linestyle='--', alpha=0.8)\n",
    "                \n",
    "                plt.title(f'Validación Reconstrucción: {objetivo} - Estación {estacion}')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Guardar gráfica en la carpeta de la variable\n",
    "                plt.savefig(os.path.join(dir_variable, f'plot_validacion_{objetivo}.png'))\n",
    "                plt.close()\n",
    "                # ---------------------------------------------------------\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      [ERROR Masking] {e}\")\n",
    "\n",
    "print(\"\\n--- PROCESO TERMINADO ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bd264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
