{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e444dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pmdarima\n",
      "  Downloading pmdarima-2.1.1-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (1.5.2)\n",
      "Collecting Cython!=0.29.18,!=0.29.31,>=0.29 (from pmdarima)\n",
      "  Downloading cython-3.2.2-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.19 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.13.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (1.15.3)\n",
      "Requirement already satisfied: statsmodels>=0.14.5 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (0.14.5)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (2.5.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=42 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (69.5.1)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pmdarima) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\sarah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels>=0.14.5->pmdarima) (1.0.2)\n",
      "Downloading pmdarima-2.1.1-cp310-cp310-win_amd64.whl (719 kB)\n",
      "   ---------------------------------------- 0.0/719.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 719.3/719.3 kB 14.7 MB/s  0:00:00\n",
      "Downloading cython-3.2.2-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/2.8 MB 26.6 MB/s  0:00:00\n",
      "Installing collected packages: Cython, pmdarima\n",
      "\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   ---------------------------------------- 0/2 [Cython]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   -------------------- ------------------- 1/2 [pmdarima]\n",
      "   ---------------------------------------- 2/2 [pmdarima]\n",
      "\n",
      "Successfully installed Cython-3.2.2 pmdarima-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install pmdarima\n",
    "%pip instal pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a8a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f15dde",
   "metadata": {},
   "source": [
    "## SEPARACION DE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10affcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: ../data/data_conagua_clasificada/Seco/NR_25037.csv\n",
      "Guardando archivos completos en ./Sarimax_dataset_splits/Seco/NR_25037/...\n",
      "Proceso finalizado para este archivo.\n",
      "\n",
      "Datos cargados: ../data/data_conagua_clasificada/Templado/NR_25033.csv\n",
      "Guardando archivos completos en ./Sarimax_dataset_splits/Templado/NR_25033/...\n",
      "Proceso finalizado para este archivo.\n",
      "\n",
      "Datos cargados: ../data/data_conagua_clasificada/Tropical/NR_25046.csv\n",
      "Guardando archivos completos en ./Sarimax_dataset_splits/Tropical/NR_25046/...\n",
      "Proceso finalizado para este archivo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_routes = ['../data/data_conagua_clasificada/Seco/NR_25037.csv',\n",
    "               '../data/data_conagua_clasificada/Templado/NR_25033.csv',\n",
    "               '../data/data_conagua_clasificada/Tropical/NR_25046.csv']\n",
    "\n",
    "output_dirs = ['./Sarimax_dataset_splits/Seco/NR_25037/',\n",
    "               './Sarimax_dataset_splits/Templado/NR_25033/',\n",
    "               './Sarimax_dataset_splits/Tropical/NR_25046/']\n",
    "\n",
    "routes = [(file_route, output_dir) for file_route, output_dir in zip(file_routes, output_dirs)]\n",
    "\n",
    "columnas_interes = ['EVAP', 'PRECIP', 'TMAX', 'TMIN'] \n",
    "\n",
    "for file_route, output_dir in routes:\n",
    "\n",
    "    df = pd.read_csv(file_route, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    print(f\"Datos cargados: {file_route}\")\n",
    "\n",
    "    df_continuo = df.asfreq('D')\n",
    "\n",
    "    columnas_existentes = []\n",
    "    for col in columnas_interes:\n",
    "        if col in df.columns:\n",
    "            df_continuo[f'mask_{col}'] = df_continuo[col].notna().astype(int)\n",
    "            columnas_existentes.append(col)\n",
    "        else:\n",
    "            print(f\"Advertencia: {col} no existe en este archivo.\")\n",
    "\n",
    "\n",
    "    n = len(df_continuo)\n",
    "    train_end = int(n * 0.7)\n",
    "    test_end = int(n * 0.9)\n",
    "\n",
    "    train = df_continuo.iloc[:train_end]\n",
    "    test = df_continuo.iloc[train_end:test_end]\n",
    "    validation = df_continuo.iloc[test_end:]\n",
    "\n",
    "    os.makedirs(os.path.join(output_dir, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'test'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'validation'), exist_ok=True)\n",
    "\n",
    "    print(f\"Guardando archivos completos en {output_dir}...\")\n",
    "    train.to_csv(os.path.join(output_dir, 'train/train_completo.csv'))\n",
    "    test.to_csv(os.path.join(output_dir, 'test/test_completo.csv'))\n",
    "    validation.to_csv(os.path.join(output_dir, 'validation/validation_completo.csv'))\n",
    "\n",
    "\n",
    "    for col in columnas_existentes:\n",
    "        cols_to_save = [col, f'mask_{col}']\n",
    "        \n",
    "        train[cols_to_save].to_csv(os.path.join(output_dir, f'train/train_{col}.csv'))\n",
    "        test[cols_to_save].to_csv(os.path.join(output_dir, f'test/test_{col}.csv'))\n",
    "        validation[cols_to_save].to_csv(os.path.join(output_dir, f'validation/validation_{col}.csv'))\n",
    "\n",
    "    print(\"Proceso finalizado para este archivo.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4204424",
   "metadata": {},
   "source": [
    "### Prueba de Estacionariedad Dickey-Fuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv\n",
      "\n",
      "An√°lisis para EVAP en ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -10.676035729060786\n",
      "Valor P: 4.0516909305182395e-19\n",
      "Valor Cr√≠tico (1%): -3.430743118305158\n",
      "Valor Cr√≠tico (5%): -2.8617137425399806\n",
      "Valor Cr√≠tico (10%): -2.566862478742594\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para PRECIP en ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -14.601906290872467\n",
      "Valor P: 4.1529441129528554e-27\n",
      "Valor Cr√≠tico (1%): -3.4307382864424487\n",
      "Valor Cr√≠tico (5%): -2.8617116071898954\n",
      "Valor Cr√≠tico (10%): -2.566861342125719\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMAX en ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -10.213647209319728\n",
      "Valor P: 5.554001572168047e-18\n",
      "Valor Cr√≠tico (1%): -3.4307380560249783\n",
      "Valor Cr√≠tico (5%): -2.861711505361178\n",
      "Valor Cr√≠tico (10%): -2.566861287923737\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMIN en ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -11.244157365712256\n",
      "Valor P: 1.7766583526260368e-20\n",
      "Valor Cr√≠tico (1%): -3.4307386094869905\n",
      "Valor Cr√≠tico (5%): -2.8617117499534124\n",
      "Valor Cr√≠tico (10%): -2.5668614181167166\n",
      "La serie es estacionaria. (d=0)\n",
      "Datos cargados: ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv\n",
      "\n",
      "An√°lisis para EVAP en ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -10.365248820902169\n",
      "Valor P: 2.339508067482748e-18\n",
      "Valor Cr√≠tico (1%): -3.430757269791305\n",
      "Valor Cr√≠tico (5%): -2.8617199965009057\n",
      "Valor Cr√≠tico (10%): -2.5668658076416646\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para PRECIP en ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -13.848659228814727\n",
      "Valor P: 7.071685698245374e-26\n",
      "Valor Cr√≠tico (1%): -3.430756712527064\n",
      "Valor Cr√≠tico (5%): -2.8617197502298937\n",
      "Valor Cr√≠tico (10%): -2.566865676554828\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMAX en ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -9.892529330791698\n",
      "Valor P: 3.5268927835184657e-17\n",
      "Valor Cr√≠tico (1%): -3.4307554768118327\n",
      "Valor Cr√≠tico (5%): -2.861719204131813\n",
      "Valor Cr√≠tico (10%): -2.566865385873996\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMIN en ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -10.474096186799388\n",
      "Valor P: 1.262130300292712e-18\n",
      "Valor Cr√≠tico (1%): -3.4307559802773167\n",
      "Valor Cr√≠tico (5%): -2.86171942662771\n",
      "Valor Cr√≠tico (10%): -2.566865504305633\n",
      "La serie es estacionaria. (d=0)\n",
      "Datos cargados: ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv\n",
      "\n",
      "An√°lisis para EVAP en ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -12.250869437869245\n",
      "Valor P: 9.547269137774015e-23\n",
      "Valor Cr√≠tico (1%): -3.430675866296457\n",
      "Valor Cr√≠tico (5%): -2.8616840214778794\n",
      "Valor Cr√≠tico (10%): -2.566846658692583\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para PRECIP en ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -16.26564367089843\n",
      "Valor P: 3.5295072528228116e-29\n",
      "Valor Cr√≠tico (1%): -3.4306664366343798\n",
      "Valor Cr√≠tico (5%): -2.8616798541199677\n",
      "Valor Cr√≠tico (10%): -2.566844440483384\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMAX en ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -11.27653516271984\n",
      "Valor P: 1.4917893110826638e-20\n",
      "Valor Cr√≠tico (1%): -3.430670327293361\n",
      "Valor Cr√≠tico (5%): -2.861681573564787\n",
      "Valor Cr√≠tico (10%): -2.5668453557124664\n",
      "La serie es estacionaria. (d=0)\n",
      "\n",
      "An√°lisis para TMIN en ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv:\n",
      "------------------------------\n",
      "Estadistica ADF: -12.502363094181378\n",
      "Valor P: 2.7883283040624e-23\n",
      "Valor Cr√≠tico (1%): -3.4306707357852657\n",
      "Valor Cr√≠tico (5%): -2.861681754094303\n",
      "Valor Cr√≠tico (10%): -2.566845451805054\n",
      "La serie es estacionaria. (d=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "file_dirs = ['./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv',\n",
    "             './Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv',\n",
    "             './Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv']\n",
    "\n",
    "default_output_dir = './data_analysis/Dickey-Fuller/'\n",
    "os.makedirs(default_output_dir, exist_ok=True)\n",
    "\n",
    "columnas_interes = ['EVAP', 'PRECIP', 'TMAX', 'TMIN']\n",
    "\n",
    "\n",
    "for file_dir in file_dirs:\n",
    "    df = pd.read_csv(file_dir, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    print(f\"Datos cargados: {file_dir}\")\n",
    "    \n",
    "    \n",
    "    estacion = 'Desconocida'\n",
    "    clasificacion = 'Desconocida'\n",
    "    \n",
    "    if 'ESTACI√ìN' in df.columns:\n",
    "        estacion = str(df['ESTACI√ìN'].iloc[0])\n",
    "        \n",
    "    if 'CLASIFICACION' in df.columns:\n",
    "        clasificacion = str(df['CLASIFICACION'].iloc[0])\n",
    "        \n",
    "        \n",
    "    output_dir = os.path.join(default_output_dir, clasificacion, estacion)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    reporte_output_dir = os.path.join(output_dir, f'reporte_ADF.txt')\n",
    "    #La prueba de Dickey-Fuller Aumentada (ADF) es una prueba estad√≠stica utilizada para determinar si una serie temporal es estacionaria o no.\n",
    "    #ADF prueba la hip√≥tesis nula de que una unidad ra√≠z est√° presente en una serie temporal.\n",
    "    #P valor bajo (generalmente ‚â§ 0.05) indica que se puede rechazar la hip√≥tesis nula, sugiriendo que la serie temporal es estacionaria.\n",
    "    #D = 0: Serie estacionaria\n",
    "    #D >= 1: Serie no estacionaria, se recomienda diferenciar.\n",
    "    \n",
    "    with open(reporte_output_dir, 'w') as reporte_file:\n",
    "        reporte_file.write(f\"Reporte de Prueba de Dickey-Fuller Aumentada (ADF) para {estacion} - {clasificacion}\\n\")\n",
    "        \n",
    "        for col in columnas_interes:\n",
    "            if col not in df.columns:\n",
    "                \n",
    "                \n",
    "                print(f\"Advertencia: {col} no existe en {file_dir}. Saltando an√°lisis.\")\n",
    "                continue\n",
    "\n",
    "            reporte_file.write(f\"An√°lisis para {col}:\\n\")\n",
    "            \n",
    "            series = df[col].dropna()\n",
    "            \n",
    "            print(f\"\\nAn√°lisis para {col} en {file_dir}:\")\n",
    "            result = adfuller(series)\n",
    "            \n",
    "            \n",
    "            # Estad√≠stica ADF \n",
    "            print(f'Estadistica ADF: {result[0]}')\n",
    "            reporte_file.write(f'Estadistica ADF: {result[0]:.4f}\\n\\n')\n",
    "            \n",
    "            # Valor P\n",
    "            print(f'Valor P: {result[1]}')        \n",
    "            reporte_file.write(f'Valor P: {result[1]:.4f}\\n\\n')\n",
    "            \n",
    "            #Valores cr√≠ticos\n",
    "            for key, value in result[4].items():\n",
    "                print(f'Valor Cr√≠tico ({key}): {value}')\n",
    "                reporte_file.write(f'Valor Cr√≠tico ({key}): {value:.4f}\\n')\n",
    "            \n",
    "            # Conclusi√≥n\n",
    "            if result[1] < 0.05:\n",
    "                valor_d = 0\n",
    "                print(\"La serie es estacionaria. (d=0)\")\n",
    "                reporte_file.write(\"La serie es estacionaria. (d=0)\\n\\n\")\n",
    "            else:\n",
    "                valor_d = 1\n",
    "                print(\"La serie no es estacionaria. Considera diferenciar. (d>=1)\")\n",
    "                reporte_file.write(\"La serie no es estacionaria. Considera diferenciar. (d>=1)\\n\\n\")\n",
    "                \n",
    "            reporte_file.write(\"\\n\")\n",
    "            reporte_file.write(f\"Resultado: Estad√≠stica ADF = {result[0]:.4f}, Valor P = {result[1]:.4f}, Valor D = {valor_d}\\n\")\n",
    "                \n",
    "                \n",
    "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "            \n",
    "            # ACF: Ayuda a identificar el t√©rmino MA (q)\n",
    "            plot_acf(series, ax=ax1, lags=40)\n",
    "            ax1.set_title(f'Funci√≥n de Autocorrelaci√≥n (ACF) para {col} en {estacion} - {clasificacion}')\n",
    "            \n",
    "            # PACF: Ayuda a identificar el t√©rmino AR (p)\n",
    "            plot_pacf(series, ax=ax2, lags=40, method='ywm')\n",
    "            ax2.set_title(f'Funci√≥n de Autocorrelaci√≥n Parcial (PACF) para {col} en {estacion} - {clasificacion}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, f'plot_ACF_PACF_{col}.png'))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bde7e",
   "metadata": {},
   "source": [
    "## Busqueda Automatica de Hiperparametros Hyndman-Khandakar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47bcd0",
   "metadata": {},
   "source": [
    "### HiperParametros Evap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5020e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando AutoARIMA para Estaci√≥n: 25037.0, Clasificaci√≥n: Seco\n",
      "Variables Ex√≥genas utilizadas: ['TMAX', 'TMIN', 'PRECIP', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Seco_NR_25037/best_model_params_EVAP.json\n",
      "\n",
      "Orden Arima Encontrada: (4, 0, 3)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25033.0, Clasificaci√≥n: Templado\n",
      "Variables Ex√≥genas utilizadas: ['TMAX', 'TMIN', 'PRECIP', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Templado_NR_25033/best_model_params_EVAP.json\n",
      "\n",
      "Orden Arima Encontrada: (3, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25046.0, Clasificaci√≥n: Tropical\n",
      "Variables Ex√≥genas utilizadas: ['TMAX', 'TMIN', 'PRECIP', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_EVAP.json\n",
      "\n",
      "Orden Arima Encontrada: (3, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Rutas Entradas y Salidas\n",
    "rutas_procesamiento = [('./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv', './data_analysis/hyperparameters/Seco_NR_25037/'),\n",
    "    ('./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv', './data_analysis/hyperparameters/Templado_NR_25033/'),\n",
    "    ('./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv', './data_analysis/hyperparameters/Tropical_NR_25046/')\n",
    "]\n",
    "\n",
    "#Busqueda de hiperparametros AutoARIMA con Fourier y Dynamic Harmonic Regression\n",
    "#Hyndman - Khandakar postula una metodolog√≠a para seleccionar autom√°ticamente los par√°metros del modelo ARIMA utilizando un enfoque basado en pruebas estad√≠sticas y criterios de informaci√≥n.\n",
    "#El procedimiento incluye:\n",
    "#1. Identificaci√≥n de la necesidad de diferenciaci√≥n (d) mediante pruebas de ra√≠z unitaria como la prueba de Dickey-Fuller aumentada.\n",
    "#2. Selecci√≥n inicial de los √≥rdenes p y q basados en la inspecci√≥n de las funciones de autocorrelaci√≥n (ACF) y autocorrelaci√≥n parcial (PACF).\n",
    "#3. B√∫squeda iterativa de los mejores par√°metros p, d, q utilizando criterios de informaci√≥n como AIC o BIC para evaluar la calidad del modelo.\n",
    "#4. Validaci√≥n cruzada y diagn√≥stico de residuos para asegurar que el modelo seleccionado es adecuado para los datos.\n",
    "    \n",
    "#Definicion de columnas objetivo y ex√≥genas\n",
    "target_col = 'EVAP'\n",
    "original_exog_cols = ['TMAX', 'TMIN', 'PRECIP']\n",
    "\n",
    "\n",
    "for input_path, output_dir in rutas_procesamiento:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {input_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #Generar Fourier con Curva anual suave\n",
    "    K_Fourier = 2\n",
    "    \n",
    "    df_fourier = df.copy()\n",
    "    \n",
    "    t = np.arange(len(df_fourier)) +1\n",
    "    \n",
    "    for k in range(1, K_Fourier + 1):\n",
    "        omega = 2 * np.pi * k / 365.25\n",
    "        df_fourier[f'sin_{k}'] = np.sin(omega * t)\n",
    "        df_fourier[f'cos_{k}'] = np.cos(omega * t)\n",
    "    \n",
    "    fourier_cols = [col for col in df_fourier.columns if 'sin_' in col or 'cos_' in col]\n",
    "    \n",
    "    \n",
    "    #Combinar Exogenas con Fourier\n",
    "    x_full = pd.concat([df_fourier[original_exog_cols], df_fourier[fourier_cols]], axis=1)\n",
    "    \n",
    "    combined = pd.concat([df_fourier[target_col], x_full], axis=1).dropna()\n",
    "    \n",
    "    y_clean = combined[target_col]\n",
    "    X_clean = combined.drop(columns=[target_col])\n",
    "    \n",
    "    \n",
    "    Estacion = 'Desconocida'\n",
    "    Clasificacion = 'Desconocida'\n",
    "    if 'ESTACI√ìN' in df.columns:\n",
    "        Estacion = str(df['ESTACI√ìN'].iloc[0])\n",
    "    if 'CLASIFICACION' in df.columns:\n",
    "        Clasificacion = str(df['CLASIFICACION'].iloc[0])\n",
    "        \n",
    "    print(f'Iniciando AutoARIMA para Estaci√≥n: {Estacion}, Clasificaci√≥n: {Clasificacion}')\n",
    "    print('Variables Ex√≥genas utilizadas:', X_clean.columns.tolist())\n",
    "    \n",
    "    model_auto = pm.auto_arima(y_clean,\n",
    "                               exogenous=X_clean,\n",
    "                               start_p=0,\n",
    "                               start_q=0,\n",
    "                               max_p=5,\n",
    "                               max_q=3,\n",
    "                               d=0,\n",
    "                               seasonal=False, #Se quita estacionalidad por presencia de Fourier\n",
    "                               trace=False,\n",
    "                               error_action='ignore',\n",
    "                               suppress_warnings=True,\n",
    "                               stepwise=True)\n",
    "    \n",
    "    resultados = {\n",
    "        \"Estacion\": Estacion,\n",
    "        \"metodo\": \"Dynamic Harmonic Regression con AutoARIMA\",\n",
    "        \"Clasificacion\": Clasificacion,\n",
    "        \"fourier_K\" : K_Fourier,\n",
    "        \"exogenous_variables\": X_clean.columns.tolist(),\n",
    "        \"best_arima_order\": model_auto.order,\n",
    "        \"AIC\": model_auto.aic(),\n",
    "        \"BIC\": model_auto.bic()\n",
    "    }\n",
    "    \n",
    "    output_json_path = os.path.join(output_dir, f'best_model_params_{target_col}.json')\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(resultados, json_file, indent=4)\n",
    "        \n",
    "    print(f'Modelo guardado en {output_json_path}\\n')\n",
    "    print(\"Orden Arima Encontrada:\", model_auto.order)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3b38f",
   "metadata": {},
   "source": [
    "### Hiperparametros Precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7693e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando AutoARIMA para Estaci√≥n: 25037.0, Clasificaci√≥n: Seco\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'TMAX', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Seco_NR_25037/best_model_params_PRECIP.json\n",
      "\n",
      "Orden Arima Encontrada: (5, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25033.0, Clasificaci√≥n: Templado\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'TMAX', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Templado_NR_25033/best_model_params_PRECIP.json\n",
      "\n",
      "Orden Arima Encontrada: (5, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25046.0, Clasificaci√≥n: Tropical\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'TMAX', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_PRECIP.json\n",
      "\n",
      "Orden Arima Encontrada: (1, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Rutas Entradas y Salidas\n",
    "rutas_procesamiento = [('./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv', './data_analysis/hyperparameters/Seco_NR_25037/'),\n",
    "    ('./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv', './data_analysis/hyperparameters/Templado_NR_25033/'),\n",
    "    ('./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv', './data_analysis/hyperparameters/Tropical_NR_25046/')\n",
    "]\n",
    "\n",
    "\n",
    "    \n",
    "#Definicion de columnas objetivo y ex√≥genas\n",
    "target_col = 'PRECIP'\n",
    "original_exog_cols = ['EVAP', 'TMAX', 'TMIN']\n",
    "\n",
    "\n",
    "for input_path, output_dir in rutas_procesamiento:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {input_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #Generar Fourier con Curva anual suave\n",
    "    K_Fourier = 2\n",
    "    \n",
    "    df_fourier = df.copy()\n",
    "    \n",
    "    t = np.arange(len(df_fourier)) +1\n",
    "    \n",
    "    for k in range(1, K_Fourier + 1):\n",
    "        omega = 2 * np.pi * k / 365.25\n",
    "        df_fourier[f'sin_{k}'] = np.sin(omega * t)\n",
    "        df_fourier[f'cos_{k}'] = np.cos(omega * t)\n",
    "    \n",
    "    fourier_cols = [col for col in df_fourier.columns if 'sin_' in col or 'cos_' in col]\n",
    "    \n",
    "    \n",
    "    #Combinar Exogenas con Fourier\n",
    "    x_full = pd.concat([df_fourier[original_exog_cols], df_fourier[fourier_cols]], axis=1)\n",
    "    \n",
    "    combined = pd.concat([df_fourier[target_col], x_full], axis=1).dropna()\n",
    "    \n",
    "    y_clean = combined[target_col]\n",
    "    X_clean = combined.drop(columns=[target_col])\n",
    "    \n",
    "    \n",
    "    Estacion = 'Desconocida'\n",
    "    Clasificacion = 'Desconocida'\n",
    "    if 'ESTACI√ìN' in df.columns:\n",
    "        Estacion = str(df['ESTACI√ìN'].iloc[0])\n",
    "    if 'CLASIFICACION' in df.columns:\n",
    "        Clasificacion = str(df['CLASIFICACION'].iloc[0])\n",
    "        \n",
    "    print(f'Iniciando AutoARIMA para Estaci√≥n: {Estacion}, Clasificaci√≥n: {Clasificacion}')\n",
    "    print('Variables Ex√≥genas utilizadas:', X_clean.columns.tolist())\n",
    "    \n",
    "    model_auto = pm.auto_arima(y_clean,\n",
    "                               exogenous=X_clean,\n",
    "                               start_p=0,\n",
    "                               start_q=0,\n",
    "                               max_p=5,\n",
    "                               max_q=3,\n",
    "                               d=0,\n",
    "                               seasonal=False, #Se quita estacionalidad por presencia de Fourier\n",
    "                               trace=False,\n",
    "                               error_action='ignore',\n",
    "                               suppress_warnings=True,\n",
    "                               stepwise=True)\n",
    "    \n",
    "    resultados = {\n",
    "        \"Estacion\": Estacion,\n",
    "        \"metodo\": \"Dynamic Harmonic Regression con AutoARIMA\",\n",
    "        \"Clasificacion\": Clasificacion,\n",
    "        \"fourier_K\" : K_Fourier,\n",
    "        \"exogenous_variables\": X_clean.columns.tolist(),\n",
    "        \"best_arima_order\": model_auto.order,\n",
    "        \"AIC\": model_auto.aic(),\n",
    "        \"BIC\": model_auto.bic()\n",
    "    }\n",
    "    \n",
    "    output_json_path = os.path.join(output_dir, f'best_model_params_{target_col}.json')\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(resultados, json_file, indent=4)\n",
    "        \n",
    "    print(f'Modelo guardado en {output_json_path}\\n')\n",
    "    print(\"Orden Arima Encontrada:\", model_auto.order)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547ee38",
   "metadata": {},
   "source": [
    "### Hiperparametros TMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04c926dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando AutoARIMA para Estaci√≥n: 25037.0, Clasificaci√≥n: Seco\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Seco_NR_25037/best_model_params_TMAX.json\n",
      "\n",
      "Orden Arima Encontrada: (4, 0, 3)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25033.0, Clasificaci√≥n: Templado\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Templado_NR_25033/best_model_params_TMAX.json\n",
      "\n",
      "Orden Arima Encontrada: (3, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25046.0, Clasificaci√≥n: Tropical\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMIN', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_TMAX.json\n",
      "\n",
      "Orden Arima Encontrada: (3, 0, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Rutas Entradas y Salidas\n",
    "rutas_procesamiento = [('./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv', './data_analysis/hyperparameters/Seco_NR_25037/'),\n",
    "    ('./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv', './data_analysis/hyperparameters/Templado_NR_25033/'),\n",
    "    ('./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv', './data_analysis/hyperparameters/Tropical_NR_25046/')\n",
    "]\n",
    "\n",
    "\n",
    "    \n",
    "#Definicion de columnas objetivo y ex√≥genas\n",
    "target_col = 'TMAX'\n",
    "original_exog_cols = ['EVAP', 'PRECIP', 'TMIN']\n",
    "\n",
    "\n",
    "for input_path, output_dir in rutas_procesamiento:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {input_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #Generar Fourier con Curva anual suave\n",
    "    K_Fourier = 2\n",
    "    \n",
    "    df_fourier = df.copy()\n",
    "    \n",
    "    t = np.arange(len(df_fourier)) +1\n",
    "    \n",
    "    for k in range(1, K_Fourier + 1):\n",
    "        omega = 2 * np.pi * k / 365.25\n",
    "        df_fourier[f'sin_{k}'] = np.sin(omega * t)\n",
    "        df_fourier[f'cos_{k}'] = np.cos(omega * t)\n",
    "    \n",
    "    fourier_cols = [col for col in df_fourier.columns if 'sin_' in col or 'cos_' in col]\n",
    "    \n",
    "    \n",
    "    #Combinar Exogenas con Fourier\n",
    "    x_full = pd.concat([df_fourier[original_exog_cols], df_fourier[fourier_cols]], axis=1)\n",
    "    \n",
    "    combined = pd.concat([df_fourier[target_col], x_full], axis=1).dropna()\n",
    "    \n",
    "    y_clean = combined[target_col]\n",
    "    X_clean = combined.drop(columns=[target_col])\n",
    "    \n",
    "    \n",
    "    Estacion = 'Desconocida'\n",
    "    Clasificacion = 'Desconocida'\n",
    "    if 'ESTACI√ìN' in df.columns:\n",
    "        Estacion = str(df['ESTACI√ìN'].iloc[0])\n",
    "    if 'CLASIFICACION' in df.columns:\n",
    "        Clasificacion = str(df['CLASIFICACION'].iloc[0])\n",
    "        \n",
    "    print(f'Iniciando AutoARIMA para Estaci√≥n: {Estacion}, Clasificaci√≥n: {Clasificacion}')\n",
    "    print('Variables Ex√≥genas utilizadas:', X_clean.columns.tolist())\n",
    "    \n",
    "    model_auto = pm.auto_arima(y_clean,\n",
    "                               exogenous=X_clean,\n",
    "                               start_p=0,\n",
    "                               start_q=0,\n",
    "                               max_p=5,\n",
    "                               max_q=3,\n",
    "                               d=0,\n",
    "                               seasonal=False, #Se quita estacionalidad por presencia de Fourier\n",
    "                               trace=False,\n",
    "                               error_action='ignore',\n",
    "                               suppress_warnings=True,\n",
    "                               stepwise=True)\n",
    "    \n",
    "    resultados = {\n",
    "        \"Estacion\": Estacion,\n",
    "        \"metodo\": \"Dynamic Harmonic Regression con AutoARIMA\",\n",
    "        \"Clasificacion\": Clasificacion,\n",
    "        \"fourier_K\" : K_Fourier,\n",
    "        \"exogenous_variables\": X_clean.columns.tolist(),\n",
    "        \"best_arima_order\": model_auto.order,\n",
    "        \"AIC\": model_auto.aic(),\n",
    "        \"BIC\": model_auto.bic()\n",
    "    }\n",
    "    \n",
    "    output_json_path = os.path.join(output_dir, f'best_model_params_{target_col}.json')\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(resultados, json_file, indent=4)\n",
    "        \n",
    "    print(f'Modelo guardado en {output_json_path}\\n')\n",
    "    print(\"Orden Arima Encontrada:\", model_auto.order)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7b711",
   "metadata": {},
   "source": [
    "### Hiperparametros TMIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb235d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando AutoARIMA para Estaci√≥n: 25037.0, Clasificaci√≥n: Seco\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMAX', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Seco_NR_25037/best_model_params_TMIN.json\n",
      "\n",
      "Orden Arima Encontrada: (4, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25033.0, Clasificaci√≥n: Templado\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMAX', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Templado_NR_25033/best_model_params_TMIN.json\n",
      "\n",
      "Orden Arima Encontrada: (2, 0, 1)\n",
      "Iniciando AutoARIMA para Estaci√≥n: 25046.0, Clasificaci√≥n: Tropical\n",
      "Variables Ex√≥genas utilizadas: ['EVAP', 'PRECIP', 'TMAX', 'sin_1', 'cos_1', 'sin_2', 'cos_2']\n",
      "Modelo guardado en ./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_TMIN.json\n",
      "\n",
      "Orden Arima Encontrada: (5, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Rutas Entradas y Salidas\n",
    "rutas_procesamiento = [('./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv', './data_analysis/hyperparameters/Seco_NR_25037/'),\n",
    "    ('./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv', './data_analysis/hyperparameters/Templado_NR_25033/'),\n",
    "    ('./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv', './data_analysis/hyperparameters/Tropical_NR_25046/')\n",
    "]\n",
    "\n",
    "\n",
    "    \n",
    "#Definicion de columnas objetivo y ex√≥genas\n",
    "target_col = 'TMIN'\n",
    "original_exog_cols = ['EVAP', 'PRECIP', 'TMAX']\n",
    "\n",
    "\n",
    "for input_path, output_dir in rutas_procesamiento:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA')\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path, parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar {input_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    #Generar Fourier con Curva anual suave\n",
    "    K_Fourier = 2\n",
    "    \n",
    "    df_fourier = df.copy()\n",
    "    \n",
    "    t = np.arange(len(df_fourier)) +1\n",
    "    \n",
    "    for k in range(1, K_Fourier + 1):\n",
    "        omega = 2 * np.pi * k / 365.25\n",
    "        df_fourier[f'sin_{k}'] = np.sin(omega * t)\n",
    "        df_fourier[f'cos_{k}'] = np.cos(omega * t)\n",
    "    \n",
    "    fourier_cols = [col for col in df_fourier.columns if 'sin_' in col or 'cos_' in col]\n",
    "    \n",
    "    \n",
    "    #Combinar Exogenas con Fourier\n",
    "    x_full = pd.concat([df_fourier[original_exog_cols], df_fourier[fourier_cols]], axis=1)\n",
    "    \n",
    "    combined = pd.concat([df_fourier[target_col], x_full], axis=1).dropna()\n",
    "    \n",
    "    y_clean = combined[target_col]\n",
    "    X_clean = combined.drop(columns=[target_col])\n",
    "    \n",
    "    \n",
    "    Estacion = 'Desconocida'\n",
    "    Clasificacion = 'Desconocida'\n",
    "    if 'ESTACI√ìN' in df.columns:\n",
    "        Estacion = str(df['ESTACI√ìN'].iloc[0])\n",
    "    if 'CLASIFICACION' in df.columns:\n",
    "        Clasificacion = str(df['CLASIFICACION'].iloc[0])\n",
    "        \n",
    "    print(f'Iniciando AutoARIMA para Estaci√≥n: {Estacion}, Clasificaci√≥n: {Clasificacion}')\n",
    "    print('Variables Ex√≥genas utilizadas:', X_clean.columns.tolist())\n",
    "    \n",
    "    model_auto = pm.auto_arima(y_clean,\n",
    "                               exogenous=X_clean,\n",
    "                               start_p=0,\n",
    "                               start_q=0,\n",
    "                               max_p=5,\n",
    "                               max_q=3,\n",
    "                               d=0,\n",
    "                               seasonal=False, #Se quita estacionalidad por presencia de Fourier\n",
    "                               trace=False,\n",
    "                               error_action='ignore',\n",
    "                               suppress_warnings=True,\n",
    "                               stepwise=True)\n",
    "    \n",
    "    resultados = {\n",
    "        \"Estacion\": Estacion,\n",
    "        \"metodo\": \"Dynamic Harmonic Regression con AutoARIMA\",\n",
    "        \"Clasificacion\": Clasificacion,\n",
    "        \"fourier_K\" : K_Fourier,\n",
    "        \"exogenous_variables\": X_clean.columns.tolist(),\n",
    "        \"best_arima_order\": model_auto.order,\n",
    "        \"AIC\": model_auto.aic(),\n",
    "        \"BIC\": model_auto.bic()\n",
    "    }\n",
    "    \n",
    "    output_json_path = os.path.join(output_dir, f'best_model_params_{target_col}.json')\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(resultados, json_file, indent=4)\n",
    "        \n",
    "    print(f'Modelo guardado en {output_json_path}\\n')\n",
    "    print(\"Orden Arima Encontrada:\", model_auto.order)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f87b24",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3832a",
   "metadata": {},
   "source": [
    "### Relleno de datos de entrenamiento con estaciones cercanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO IMPUTACI√ìN ESPACIAL CON DIRECTORIOS DE APOYO ---\n",
      "\n",
      "==================================================\n",
      "üìÇ Procesando Objetivo: ./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv\n",
      "üîé Buscando ayuda en: ../data/data_conagua_clasificada/Seco\n",
      "   Huecos iniciales a llenar: 6925\n",
      "   Vecinos √∫tiles encontrados: 0\n",
      "   ‚ö†Ô∏è No se encontraron vecinos que cumplan los criterios de Distancia/Altitud.\n",
      "\n",
      "==================================================\n",
      "üìÇ Procesando Objetivo: ./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv\n",
      "üîé Buscando ayuda en: ../data/data_conagua_clasificada/Templado\n",
      "   Huecos iniciales a llenar: 1515\n",
      "   Vecinos √∫tiles encontrados: 0\n",
      "   ‚ö†Ô∏è No se encontraron vecinos que cumplan los criterios de Distancia/Altitud.\n",
      "\n",
      "==================================================\n",
      "üìÇ Procesando Objetivo: ./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv\n",
      "üîé Buscando ayuda en: ../data/data_conagua_clasificada/Tropical\n",
      "   Huecos iniciales a llenar: 3873\n",
      "   Vecinos √∫tiles encontrados: 0\n",
      "   ‚ö†Ô∏è No se encontraron vecinos que cumplan los criterios de Distancia/Altitud.\n",
      "\n",
      "--- PROCESO TERMINADO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "\n",
    "rutas_procesamiento = [\n",
    "    ('./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv', '../data/data_conagua_clasificada/Seco'),\n",
    "    ('./Sarimax_dataset_splits/Templado/NR_25033/train/train_completo.csv', '../data/data_conagua_clasificada/Templado'),\n",
    "    ('./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv', '../data/data_conagua_clasificada/Tropical')\n",
    "]\n",
    "\n",
    "MAX_DIST_KM = 50.0     \n",
    "MAX_ALT_DIFF_M = 500.0 \n",
    "\n",
    "\n",
    "for target_path, neighbors_dir in rutas_procesamiento:\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"Procesando Objetivo: {target_path}\")\n",
    "    print(f\"Buscando ayuda en: {neighbors_dir}\")\n",
    "    \n",
    "    # 1. CARGAR DATOS DEL OBJETIVO (TARGET)\n",
    "    try:\n",
    "        df_target = pd.read_csv(target_path, parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando el archivo objetivo: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        target_meta = df_target.iloc[0]\n",
    "        lat_t = float(target_meta['LATITUD'])\n",
    "        lon_t = float(target_meta['LONGITUD'])\n",
    "        alt_t = float(target_meta['ALTITUD'])\n",
    "        code_t = str(target_meta['ESTACI√ìN'])\n",
    "    except:\n",
    "        print(\"No se pudieron leer los metadatos .\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    cols_interes = ['PRECIP', 'TMAX', 'TMIN', 'EVAP']\n",
    "    cols_existentes = [c for c in cols_interes if c in df_target.columns]\n",
    "    \n",
    "    nans_iniciales = df_target[cols_existentes].isna().sum().sum()\n",
    "    print(f\"Huecos iniciales a llenar: {nans_iniciales}\")\n",
    "    \n",
    "    if nans_iniciales == 0:\n",
    "        print(\"-> Archivo completo. No requiere imputaci√≥n.\")\n",
    "        continue\n",
    "\n",
    "    # ESCANEAR CARPETA DE VECINOS (Discovery)\n",
    "    posibles_vecinos = [f for f in os.listdir(neighbors_dir) if f.endswith('.csv')]\n",
    "    vecinos_validos = []\n",
    "    for archivo_vecino in posibles_vecinos:\n",
    "        path_vecino = os.path.join(neighbors_dir, archivo_vecino)\n",
    "        if str(code_t) in archivo_vecino: \n",
    "            continue \n",
    "\n",
    "        try:\n",
    "            df_temp = pd.read_csv(path_vecino, nrows=5)\n",
    "            \n",
    "            if 'LATITUD' not in df_temp.columns: continue\n",
    "\n",
    "            lat_v = float(df_temp['LATITUD'].iloc[0])\n",
    "            lon_v = float(df_temp['LONGITUD'].iloc[0])\n",
    "            alt_v = float(df_temp['ALTITUD'].iloc[0])\n",
    "            code_v = str(df_temp['CODIGO'].iloc[0])\n",
    "\n",
    "            # Haversine inline\n",
    "            r_lat_t, r_lon_t = radians(lat_t), radians(lon_t)\n",
    "            r_lat_v, r_lon_v = radians(lat_v), radians(lon_v)\n",
    "            \n",
    "            dlon = r_lon_v - r_lon_t\n",
    "            dlat = r_lat_v - r_lat_t\n",
    "            \n",
    "            a = sin(dlat/2)**2 + cos(r_lat_t) * cos(r_lat_v) * sin(dlon/2)**2\n",
    "            c = 2 * asin(sqrt(a))\n",
    "            dist_km = 6371 * c # Radio tierra\n",
    "\n",
    "            #Diferencia de altura\n",
    "            alt_diff = abs(alt_t - alt_v)\n",
    "\n",
    "            #Filtrar por criterios\n",
    "            if dist_km <= MAX_DIST_KM and alt_diff <= MAX_ALT_DIFF_M:\n",
    "                vecinos_validos.append({\n",
    "                    'path': path_vecino,\n",
    "                    'ESTACI√ìN': code_v,\n",
    "                    'distancia': dist_km,\n",
    "                    'alt_diff': alt_diff\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            continue \n",
    "\n",
    "\n",
    "    vecinos_validos.sort(key=lambda x: x['distancia'])\n",
    "    \n",
    "    print(f\"Vecinos √∫tiles encontrados: {len(vecinos_validos)}\")\n",
    "    \n",
    "    if len(vecinos_validos) > 0:\n",
    "        print(\"   Iniciando transfusi√≥n de datos...\")\n",
    "        \n",
    "        for vecino in vecinos_validos:\n",
    "            # Verificamos si aun faltan datos\n",
    "            if df_target[cols_existentes].isna().sum().sum() == 0:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                df_donador = pd.read_csv(vecino['path'], parse_dates=['FECHA'], index_col='FECHA').asfreq('D')\n",
    "                \n",
    "                print(f\"     -> Tomando datos de: {vecino['ESTACI√ìN']} (Dist: {vecino['distancia']:.1f}km)\")\n",
    "                \n",
    "                for col in cols_existentes:\n",
    "                    if col in df_donador.columns:\n",
    "                        df_target[col] = df_target[col].fillna(df_donador[col])\n",
    "                        \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        nans_finales = df_target[cols_existentes].isna().sum().sum()\n",
    "        recuperados = nans_iniciales - nans_finales\n",
    "        \n",
    "        if recuperados > 0:\n",
    "            print(f\"IMPUTACI√ìN EXITOSA: Se recuperaron {recuperados} registros.\")\n",
    "            df_target.to_csv(target_path)\n",
    "            print(f\"      Archivo actualizado: {target_path}\")\n",
    "        else:\n",
    "            print(\"Los vecinos encontrados no ten√≠an datos para las fechas faltantes.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No se encontraron vecinos que cumplan los criterios de Distancia/Altitud.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c9324",
   "metadata": {},
   "source": [
    "## Entrenamiento SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e559f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO RECALIBRACI√ìN (TIPO NUM√âRICO FORZADO) ---\n",
      "\n",
      "Procesando: Seco - 25037 - TMAX...\n",
      "   -> Aviso: Se encontraron NaNs tras la conversi√≥n. Rellenando...\n",
      "   -> Tipos de datos corregidos. X shape: (18557, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y = y.fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:85: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Resultado: CONVERGENCIA EXITOSA\n",
      "   -> Guardado en: ./models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_TMAX.pkl\n",
      "\n",
      "Procesando: Tropical - 25046 - TMAX...\n",
      "   -> Aviso: Se encontraron NaNs tras la conversi√≥n. Rellenando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y = y.fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:85: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Tipos de datos corregidos. X shape: (21391, 3)\n",
      "   -> Resultado: CONVERGENCIA EXITOSA\n",
      "   -> Guardado en: ./models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMAX.pkl\n",
      "\n",
      "Procesando: Tropical - 25046 - TMIN...\n",
      "   -> Aviso: Se encontraron NaNs tras la conversi√≥n. Rellenando...\n",
      "   -> Tipos de datos corregidos. X shape: (21391, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  y = y.fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\Sarah\\AppData\\Local\\Temp\\ipykernel_30440\\3361428809.py:85: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Resultado: CONVERGENCIA EXITOSA\n",
      "   -> Guardado en: ./models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMIN.pkl\n",
      "\n",
      "--- FIN ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignorar warnings\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "\n",
    "casos_a_recalibrar = [\n",
    "    {\n",
    "        \"region\": \"Seco\",\n",
    "        \"station\": \"25037\",\n",
    "        \"target\": \"TMAX\",\n",
    "        \"data_path\": \"./Sarimax_dataset_splits/Seco/NR_25037/train/train_completo.csv\",\n",
    "        \"json_path\": \"./data_analysis/hyperparameters/Seco_NR_25037/best_model_params_TMAX.json\",\n",
    "        \"model_out\": \"./models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_TMAX.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"region\": \"Tropical\",\n",
    "        \"station\": \"25046\",\n",
    "        \"target\": \"TMAX\",\n",
    "        \"data_path\": \"./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv\",\n",
    "        \"json_path\": \"./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_TMAX.json\",\n",
    "        \"model_out\": \"./models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMAX.pkl\"\n",
    "    },\n",
    "    {\n",
    "        \"region\": \"Tropical\",\n",
    "        \"station\": \"25046\",\n",
    "        \"target\": \"TMIN\",\n",
    "        \"data_path\": \"./Sarimax_dataset_splits/Tropical/NR_25046/train/train_completo.csv\",\n",
    "        \"json_path\": \"./data_analysis/hyperparameters/Tropical_NR_25046/best_model_params_TMIN.json\",\n",
    "        \"model_out\": \"./models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMIN.pkl\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"--- INICIANDO RECALIBRACI√ìN (TIPO NUM√âRICO FORZADO) ---\\n\")\n",
    "\n",
    "for caso in casos_a_recalibrar:\n",
    "    try:\n",
    "        print(f\"Procesando: {caso['region']} - {caso['station']} - {caso['target']}...\")\n",
    "        \n",
    "        # 1. Cargar Datos\n",
    "        df = pd.read_csv(caso['data_path'], index_col=0, parse_dates=True)\n",
    "        df = df.asfreq('D')\n",
    "        \n",
    "        # 2. Cargar Hiperpar√°metros para saber qu√© columnas usar\n",
    "        with open(caso['json_path'], 'r') as f:\n",
    "            params = json.load(f)\n",
    "        \n",
    "        order = tuple(params['best_arima_order'])\n",
    "        seasonal_order = tuple(params.get('seasonal_order', (0, 0, 0, 0)))\n",
    "        \n",
    "        \n",
    "        # A. Seleccionar Target\n",
    "        y_raw = df[caso['target']]\n",
    "        # Forzar a num√©rico \n",
    "        y = pd.to_numeric(y_raw, errors='coerce').astype(float)\n",
    "        \n",
    "        # B. Seleccionar variables Exogenas\n",
    "        cols_exogenas = params.get('exogenous_variables', [])\n",
    "        \n",
    "        if not cols_exogenas:\n",
    "            X_raw = df.drop(columns=[caso['target']])\n",
    "        else:\n",
    "\n",
    "            cols_validas = [c for c in cols_exogenas if c in df.columns]\n",
    "            X_raw = df[cols_validas]\n",
    "\n",
    "        # Forzar todo X a num√©rico\n",
    "        X = X_raw.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "        \n",
    "        # C. Limpieza final de NaNs \n",
    "        if y.isnull().any() or X.isnull().any().any():\n",
    "            print(\"   -> Aviso: Se encontraron NaNs tras la conversi√≥n. Rellenando...\")\n",
    "            y = y.fillna(method='ffill').fillna(method='bfill')\n",
    "            X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        print(f\"   -> Tipos de datos corregidos. X shape: {X.shape}\")\n",
    "        \n",
    "        # 3. Definir Modelo\n",
    "        model = sm.tsa.SARIMAX(\n",
    "            endog=y,\n",
    "            exog=X,\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        # 4. Entrenar\n",
    "        model_fit = model.fit(disp=False, maxiter=600, method='lbfgs')\n",
    "        \n",
    "        if model_fit.mle_retvals['converged']:\n",
    "            estado = \"CONVERGENCIA EXITOSA\"\n",
    "        else:\n",
    "            print(\"   -> Advertencia: lbfgs no convergi√≥. Intentando 'powell'...\")\n",
    "            model_fit = model.fit(disp=False, maxiter=1000, method='powell')\n",
    "            estado = \"SEGUNDO INTENTO FINALIZADO\"\n",
    "            \n",
    "        print(f\"   -> Resultado: {estado}\")\n",
    "        \n",
    "        # 5. Guardar\n",
    "        joblib.dump(model_fit, caso['model_out'])\n",
    "        print(f\"   -> Guardado en: {caso['model_out']}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"   -> ERROR CR√çTICO: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d8693",
   "metadata": {},
   "source": [
    "## Testing del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e413e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERANDO REPORTES VISUALES LINEALES (SIN FUNCIONES) ---\n",
      "\n",
      "Procesando: Seco - TMAX...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Seco - TMIN...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Seco - PRECIP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Seco - EVAP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Tropical - TMAX...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Tropical - TMIN...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Tropical - PRECIP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Tropical - EVAP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Templado - TMAX...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Templado - TMIN...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Templado - PRECIP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "Procesando: Templado - EVAP...\n",
      "   -> Tabla guardada\n",
      "   -> Gr√°fica RMSE guardada\n",
      "\n",
      "--- PROCESO TERMINADO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "casos_evaluacion = [\n",
    "    # SECO (25037)\n",
    "    {\"region\": \"Seco\", \"station\": \"25037\", \"target\": \"TMAX\"},\n",
    "    {\"region\": \"Seco\", \"station\": \"25037\", \"target\": \"TMIN\"},\n",
    "    {\"region\": \"Seco\", \"station\": \"25037\", \"target\": \"PRECIP\"},\n",
    "    {\"region\": \"Seco\", \"station\": \"25037\", \"target\": \"EVAP\"},\n",
    "    # TROPICAL (25046)\n",
    "    {\"region\": \"Tropical\", \"station\": \"25046\", \"target\": \"TMAX\"},\n",
    "    {\"region\": \"Tropical\", \"station\": \"25046\", \"target\": \"TMIN\"},\n",
    "    {\"region\": \"Tropical\", \"station\": \"25046\", \"target\": \"PRECIP\"},\n",
    "    {\"region\": \"Tropical\", \"station\": \"25046\", \"target\": \"EVAP\"},\n",
    "    # TEMPLADO (25033)\n",
    "    {\"region\": \"Templado\", \"station\": \"25033\", \"target\": \"TMAX\"},\n",
    "    {\"region\": \"Templado\", \"station\": \"25033\", \"target\": \"TMIN\"},\n",
    "    {\"region\": \"Templado\", \"station\": \"25033\", \"target\": \"PRECIP\"},\n",
    "    {\"region\": \"Templado\", \"station\": \"25033\", \"target\": \"EVAP\"},\n",
    "]\n",
    "\n",
    "# Definici√≥n de rangos (Bins)\n",
    "bins = [0, 3, 7, 31, 180, 365, 730, float('inf')]\n",
    "labels = ['1-3 Dias', '4-7 Dias', '8-31 Dias', '32-180 Dias', '181-365 Dias', '1-2 A√±os', '>2 A√±os']\n",
    "\n",
    "print(\"--- GENERANDO REPORTES VISUALES LINEALES (SIN FUNCIONES) ---\\n\")\n",
    "\n",
    "for caso in casos_evaluacion:\n",
    "    try:\n",
    "        region = caso['region']\n",
    "        station = caso['station']\n",
    "        target = caso['target']\n",
    "        \n",
    "        # Rutas\n",
    "        test_path = f\"./Sarimax_dataset_splits/{region}/NR_{station}/test/test_completo.csv\"\n",
    "        model_path = f\"./models/trained_models/SARIMAX/SARIMAX_{station}.0_{region}_{target}.pkl\"\n",
    "        output_route = f\"./data_analysis/Performance_metrics_plots/SARIMAX/{region}/NR_{station}/{target}/test\"\n",
    "        os.makedirs(output_route, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"[SALTAR] Modelo no encontrado: {target} - {region}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Procesando: {region} - {target}...\")\n",
    "        df_test = pd.read_csv(test_path, index_col=0, parse_dates=True).asfreq('D')\n",
    "        df_test = df_test.apply(pd.to_numeric, errors='coerce').ffill().bfill()\n",
    "        model_fit = joblib.load(model_path)\n",
    "        \n",
    "        # Generaci√≥n de Fourier \n",
    "        exog_needed = [x for x in model_fit.model.exog_names if x != 'const']\n",
    "        \n",
    "        if any('sin_' in col for col in exog_needed):\n",
    "            day_of_year = df_test.index.dayofyear\n",
    "            for k in [1, 2]:\n",
    "                df_test[f'sin_{k}'] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n",
    "                df_test[f'cos_{k}'] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n",
    "\n",
    "        if len(exog_needed) > 0:\n",
    "            for c in exog_needed:\n",
    "                if c not in df_test.columns: df_test[c] = 0\n",
    "            X_test = df_test[exog_needed]\n",
    "        else:\n",
    "            X_test = None\n",
    "\n",
    "        # Predicci√≥n\n",
    "        pred_result = model_fit.get_prediction(start=df_test.index[0], end=df_test.index[-1], exog=X_test)\n",
    "        \n",
    "        # Preparar DataFrame Base\n",
    "        df_eval = pd.DataFrame({\n",
    "            'y': df_test[target],\n",
    "            'yhat': pred_result.predicted_mean\n",
    "        })\n",
    "        \n",
    "        # Calcular Horizonte y Rangos\n",
    "        df_eval['horizon_days'] = np.arange(1, len(df_eval) + 1)\n",
    "        df_eval['range'] = pd.cut(df_eval['horizon_days'], bins=bins, labels=labels)\n",
    "        \n",
    "        # Iteramos manualmente por los labels para llenar la lista\n",
    "        metrics_list = []\n",
    "        \n",
    "        for label in labels:\n",
    "            # Filtramos el pedazo de datos correspondiente a este rango\n",
    "            subset = df_eval[df_eval['range'] == label]\n",
    "            \n",
    "            if len(subset) > 0:\n",
    "                y_true = subset['y'].values\n",
    "                y_pred = subset['yhat'].values\n",
    "                \n",
    "                # RMSE / MSE\n",
    "                mse = np.mean((y_true - y_pred) ** 2)\n",
    "                rmse = np.sqrt(mse)\n",
    "                \n",
    "                # MAE\n",
    "                mae = np.mean(np.abs(y_true - y_pred))\n",
    "                \n",
    "                # MAPE (Manejo manual de ceros)\n",
    "                mask_nonzero = y_true != 0\n",
    "                if np.sum(mask_nonzero) > 0:\n",
    "                    mape = np.mean(np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero]))\n",
    "                else:\n",
    "                    mape = 0.0\n",
    "                \n",
    "                count = len(y_true)\n",
    "            else:\n",
    "                mse, rmse, mae, mape, count = 0, 0, 0, 0, 0\n",
    "            \n",
    "            # Agregamos a la lista\n",
    "            metrics_list.append({\n",
    "                'range': label,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'MAPE': mape,\n",
    "                'Count': count\n",
    "            })\n",
    "            \n",
    "        # Convertimos la lista a DataFrame\n",
    "        df_grouped_metrics = pd.DataFrame(metrics_list)\n",
    "\n",
    "        #Generacion de reportes visuales\n",
    "        display_df = df_grouped_metrics.copy()\n",
    "        # Formateo de strings para visualizaci√≥n\n",
    "        for c in ['MSE', 'RMSE', 'MAE', 'MAPE']:\n",
    "            display_df[c] = display_df[c].apply(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"-\")\n",
    "            \n",
    "        rows, cols = display_df.shape\n",
    "        fig_w = max(8, min(24, 1.5 * cols))\n",
    "        fig_h = max(2, 0.5 * rows)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table = ax.table(cellText=display_df.values, colLabels=display_df.columns, loc='center', cellLoc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 1.5)\n",
    "        \n",
    "        png_table = os.path.join(output_route, f\"metrics_table.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(png_table, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        print(f\"   -> Tabla guardada\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Filtramos ceros para que no grafique barras vac√≠as\n",
    "        plot_data = df_grouped_metrics[df_grouped_metrics['Count'] > 0]\n",
    "        \n",
    "        bars = plt.bar(plot_data['range'], plot_data['RMSE'], color='skyblue', edgecolor='black')\n",
    "        \n",
    "        plt.title(f'RMSE por Horizonte - {region} {target}')\n",
    "        plt.xlabel('Rango de Dias (Horizonte)')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}',\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "        png_plot = os.path.join(output_route, f\"RMSE_bar_plot.png\")\n",
    "        plt.savefig(png_plot, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"   -> Grafica RMSE guardada\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR CR√çTICO] en {target}: {str(e)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f154f",
   "metadata": {},
   "source": [
    "### Medir Precision con Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a8bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO EVALUACI√ìN VISUAL SARIMAX (VALIDACI√ìN) ---\n",
      "\n",
      "--> Cargando datos para: Seco - NR_25037\n",
      "    Evaluando modelo: EVAP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Seco/NR_25037/EVAP/\n",
      "    Evaluando modelo: PRECIP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Seco/NR_25037/PRECIP/\n",
      "    Evaluando modelo: TMAX...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Seco/NR_25037/TMAX/\n",
      "    Evaluando modelo: TMIN...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Seco/NR_25037/TMIN/\n",
      "--> Cargando datos para: Templado - NR_25033\n",
      "    Evaluando modelo: EVAP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Templado/NR_25033/EVAP/\n",
      "    Evaluando modelo: PRECIP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Templado/NR_25033/PRECIP/\n",
      "    Evaluando modelo: TMAX...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Templado/NR_25033/TMAX/\n",
      "    Evaluando modelo: TMIN...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Templado/NR_25033/TMIN/\n",
      "--> Cargando datos para: Tropical - NR_25046\n",
      "    Evaluando modelo: EVAP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Tropical/NR_25046/EVAP/\n",
      "    Evaluando modelo: PRECIP...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Tropical/NR_25046/PRECIP/\n",
      "    Evaluando modelo: TMAX...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Tropical/NR_25046/TMAX/\n",
      "    Evaluando modelo: TMIN...\n",
      "      -> OK. Reportes guardados en ./data_analysis/Performance_metrics_plots/SARIMAX_VAL/Tropical/NR_25046/TMIN/\n",
      "\n",
      "--- PROCESO TERMINADO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "bins = [0, 3, 7, 31, 180, 365, 730, float('inf')]\n",
    "labels = ['1-3 Dias', '4-7 Dias', '8-31 Dias', '32-180 Dias', '181-365 Dias', '1-2 A√±os', '>2 A√±os']\n",
    "\n",
    "validation_route = {\n",
    "    'Seco': {\n",
    "        'NR_25037': {'validation': './Sarimax_dataset_splits/Seco/NR_25037/validation/validation_completo.csv',\n",
    "                     'models':['./models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_EVAP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_PRECIP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_TMAX.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25037.0_Seco_TMIN.pkl']}\n",
    "    },\n",
    "    'Templado': {\n",
    "        'NR_25033': {'validation': './Sarimax_dataset_splits/Templado/NR_25033/validation/validation_completo.csv',\n",
    "                     'models':['./models/trained_models/SARIMAX/SARIMAX_25033.0_Templado_EVAP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25033.0_Templado_PRECIP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25033.0_Templado_TMAX.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25033.0_Templado_TMIN.pkl']}\n",
    "    },\n",
    "    'Tropical': {\n",
    "        'NR_25046': {'validation': './Sarimax_dataset_splits/Tropical/NR_25046/validation/validation_completo.csv',\n",
    "                     'models':['./models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_EVAP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_PRECIP.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMAX.pkl',\n",
    "                               './models/trained_models/SARIMAX/SARIMAX_25046.0_Tropical_TMIN.pkl']}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"--- INICIANDO EVALUACI√ìN VISUAL SARIMAX (VALIDACI√ìN) ---\\n\")\n",
    "\n",
    "for region, stations in validation_route.items():\n",
    "    for station_key, data in stations.items():\n",
    "        val_path = data['validation']\n",
    "        model_list = data['models']\n",
    "        \n",
    "        print(f\"--> Cargando datos para: {region} - {station_key}\")\n",
    "        \n",
    "        if not os.path.exists(val_path):\n",
    "            print(f\"    [ERROR] No existe el archivo: {val_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df_raw = pd.read_csv(val_path, index_col=0, parse_dates=True).asfreq('D')\n",
    "            df_raw = df_raw.apply(pd.to_numeric, errors='coerce') \n",
    "            \n",
    "            # Relleno simple solo para alimentar el modelo (el modelo necesita continuidad)\n",
    "            df_filled = df_raw.copy().ffill().bfill()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Al leer CSV: {e}\")\n",
    "            continue\n",
    "\n",
    "        for model_path in model_list:\n",
    "            if not os.path.exists(model_path):\n",
    "                continue\n",
    "            \n",
    "            filename = os.path.basename(model_path)\n",
    "            target = filename.split('_')[-1].replace('.pkl', '')\n",
    "            \n",
    "            if target not in df_filled.columns:\n",
    "                continue\n",
    "\n",
    "            print(f\"    Evaluando modelo: {target}...\")\n",
    "            \n",
    "            # Carpeta de salida\n",
    "            output_route = f\"./data_analysis/Performance_metrics_plots/SARIMAX_VAL/{region}/{station_key}/{target}/\"\n",
    "            os.makedirs(output_route, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                # 1. Cargar Modelo\n",
    "                model_fit = joblib.load(model_path)\n",
    "                df_run = df_filled.copy()\n",
    "                # 2. Preparar Exogenas incluyendo Fourier\n",
    "                exog_needed = [x for x in model_fit.model.exog_names if x != 'const']\n",
    "                if any('sin_' in col for col in exog_needed):\n",
    "                    day_of_year = df_run.index.dayofyear\n",
    "                    for k in [1, 2]:\n",
    "                        df_run[f'sin_{k}'] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n",
    "                        df_run[f'cos_{k}'] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n",
    "                \n",
    "                if len(exog_needed) > 0:\n",
    "                    # Asegurar que existan todas las columnas ex√≥genas\n",
    "                    for c in exog_needed:\n",
    "                        if c not in df_run.columns: df_run[c] = 0\n",
    "                    X_exog = df_run[exog_needed]\n",
    "                else:\n",
    "                    X_exog = None\n",
    "                \n",
    "                # 3. Aplicar modelo (Validation / Forecast one-step ahead)\n",
    "                # 'apply' extiende el filtro de Kalman a los nuevos datos sin reentrenar par√°metros\n",
    "                new_results = model_fit.apply(df_run[target], exog=X_exog)\n",
    "                y_pred_values = new_results.fittedvalues\n",
    "                \n",
    "                # 4. Crear DataFrame de Evaluaci√≥n\n",
    "                df_eval = pd.DataFrame({\n",
    "                    'y_real': df_raw[target], # Usamos el RAW (con huecos reales si los hay) como verdad\n",
    "                    'y_pred': y_pred_values\n",
    "                })\n",
    "                \n",
    "                # Generar Horizontes\n",
    "                df_eval['horizon_days'] = np.arange(1, len(df_eval) + 1)\n",
    "                df_eval['range'] = pd.cut(df_eval['horizon_days'], bins=bins, labels=labels)\n",
    "\n",
    "                # Guardar Predicciones\n",
    "                csv_pred_path = os.path.join(output_route, f\"predictions_val_{target}.csv\")\n",
    "                df_eval.to_csv(csv_pred_path)\n",
    "                \n",
    "                # Filtrar para m√©tricas (donde exista dato real)\n",
    "                df_eval_valid = df_eval.dropna(subset=['y_real'])\n",
    "\n",
    "                # 5. Calcular M√©tricas por Horizonte\n",
    "                metrics_list = []\n",
    "                for label in labels:\n",
    "                    subset = df_eval_valid[df_eval_valid['range'] == label]\n",
    "                    \n",
    "                    if len(subset) > 0:\n",
    "                        y_true = subset['y_real'].values\n",
    "                        y_pred = subset['y_pred'].values\n",
    "                        \n",
    "                        mse = np.mean((y_true - y_pred) ** 2)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        mae = np.mean(np.abs(y_true - y_pred))\n",
    "                        \n",
    "                        mask_nonzero = y_true != 0\n",
    "                        if np.sum(mask_nonzero) > 0:\n",
    "                            mape = np.mean(np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero]))\n",
    "                        else:\n",
    "                            mape = 0.0\n",
    "                        count = len(y_true)\n",
    "                    else:\n",
    "                        mse, rmse, mae, mape, count = 0, 0, 0, 0, 0\n",
    "                    \n",
    "                    metrics_list.append({'range': label, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'Count': count})\n",
    "                \n",
    "                df_grouped_metrics = pd.DataFrame(metrics_list)\n",
    "\n",
    "                # Guardar CSV Resumen\n",
    "                csv_metrics_path = os.path.join(output_route, f\"metrics_summary_{target}.csv\")\n",
    "                df_grouped_metrics.to_csv(csv_metrics_path, index=False)\n",
    "\n",
    "                # 6. Generar Gr√°ficos\n",
    "                \n",
    "                # A. Tabla Visual de M√©tricas\n",
    "                display_df = df_grouped_metrics.copy()\n",
    "                for c in ['MSE', 'RMSE', 'MAE', 'MAPE']:\n",
    "                    display_df[c] = display_df[c].apply(lambda x: f\"{x:.4f}\" if pd.notnull(x) else \"-\")\n",
    "                \n",
    "                rows, cols = display_df.shape\n",
    "                fig, ax = plt.subplots(figsize=(max(8, min(24, 1.5 * cols)), max(2, 0.5 * rows)))\n",
    "                ax.axis('off')\n",
    "                table = ax.table(cellText=display_df.values, colLabels=display_df.columns, loc='center', cellLoc='center')\n",
    "                table.auto_set_font_size(False)\n",
    "                table.set_fontsize(10)\n",
    "                table.scale(1, 1.5)\n",
    "                plt.title(f\"M√©tricas Validaci√≥n - {target}\", y=1.1)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(output_route, \"metrics_table_VAL.png\"), dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "                # B. Gr√°fico de Barras (RMSE por Horizonte)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plot_data = df_grouped_metrics[df_grouped_metrics['Count'] > 0]\n",
    "                if not plot_data.empty:\n",
    "                    bars = plt.bar(plot_data['range'], plot_data['RMSE'], color='skyblue', edgecolor='black')\n",
    "                    plt.title(f'Evoluci√≥n del Error (RMSE) - {region} - {target}')\n",
    "                    plt.xlabel('Horizonte de Tiempo')\n",
    "                    plt.ylabel('RMSE')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    # Etiquetas sobre las barras\n",
    "                    for bar in bars:\n",
    "                        height = bar.get_height()\n",
    "                        plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}', ha='center', va='bottom')\n",
    "                else:\n",
    "                    plt.text(0.5, 0.5, 'Sin datos v√°lidos', ha='center')\n",
    "                    \n",
    "                plt.savefig(os.path.join(output_route, \"RMSE_bar_plot_VAL.png\"), bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # C. Gr√°fico de L√≠nea (Real vs Predicho) - NUEVO (Igual que TensorFlow)\n",
    "                plt.figure(figsize=(12, 5))\n",
    "                # Graficar primeros 150 d√≠as o todo si es menos\n",
    "                limit = 150\n",
    "                subset_plot = df_eval_valid.head(limit)\n",
    "                \n",
    "                if not subset_plot.empty:\n",
    "                    plt.plot(subset_plot.index, subset_plot['y_real'], label='Real', color='blue', alpha=0.6)\n",
    "                    plt.plot(subset_plot.index, subset_plot['y_pred'], label='Predicci√≥n SARIMAX', color='orange', linestyle='--', alpha=0.8)\n",
    "                    plt.title(f'Predicci√≥n vs Realidad (Primeros {limit} d√≠as) - {target}')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.xlabel('Fecha')\n",
    "                    plt.ylabel(target)\n",
    "                else:\n",
    "                    plt.text(0.5, 0.5, 'Sin datos para graficar', ha='center')\n",
    "\n",
    "                plt.savefig(os.path.join(output_route, f\"line_plot_comparison_VAL.png\"), bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"      -> OK. Reportes guardados en {output_route}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      [ERROR CR√çTICO] Evaluando {target}: {e}\")\n",
    "\n",
    "print(\"\\n--- PROCESO TERMINADO ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
